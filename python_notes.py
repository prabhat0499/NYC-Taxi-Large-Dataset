# -*- coding: utf-8 -*-
"""Python-Notes

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1a8opjJqELLuhhib4-BP_ghIG-5CgXPiQ?resourcekey=0-FgZXjYdSNBN_B95bqGF29Q

#### Engineering Analayst - T&S
*   Proficiency in data extraction, manipulation, and analysis using various tools, including Python scripting. Therefore, expect questions focusing on practical application rather than Python trivia.
* The difficulty will depend on the seniority level.
For example, you might be asked to write a Python script to perform a specific data manipulation task or solve a problem using Python's data science libraries.
* Reviewing common data analysis tasks and Python libraries like Pandas and NumPy would be beneficial.
Analyze large datasets to identify trends, patterns, and anomalies that may indicate abuse or quality issues within Google Search.
Data analysis, including identifying trends, generating summary statistics, and drawing insights from quantitative and qualitative data.

##### Key Takeaways:
1.   They’re looking for real-world applications of Python, not theoretical questions or trivia (e.g., not "What does the yield keyword do?" but more like "Clean this dataset using Python")
2. Tools You Should Be Comfortable With : Libraries like Pandas (for data wrangling) and NumPy (for numerical operations).
Customized by Experience Level: If you're applying for a junior or entry-level role, the tasks may be simpler (e.g., filtering data, basic group-by operations).
3. Review common data analysis tasks: data cleaning, merging datasets, aggregations, handling missing values, etc.
Brush up on Pandas and NumPy functions and best practices.

###### Pointers

* Data Manipulation : Data Cleaning - Missing values, NA values, merging datasets, concat
* Filtering and Sorting - filter rows on conditions
* Groupby and Aggregation - mean, median, mode
* Datetime Handling - strings to datetime
* Pivot Tables and Crosstabs - pivot tavbles to summarize data
* Text Data Processing
* Using NumPy - array ops

Addtional areas:

*   Data read-write: F1, CNS, Spanner, sheets
*   Data interpretation/patterns - Calculate statsitics on a dataset - mean/median/modes, unique values, duplicate values
*   Data visualisation - graphs/plots by different slices of data, correlations (metrics).
* Large datasets - optimizing techniques
* Gemini - data viz

##### Note
* read and write data from different sources like - Spanner database, F1,, CNS and Sheets.
* Visualise your findings and correlations
* Breakdown your complex problems into smaller components.

---

* Skill Area	What You Can Do With This Dataset
1. Data Cleaning	Handle missing/NA values in location, fare, or datetime columns.
2. Filtering & Sorting	Filter rides by payment type, passenger count, long vs short rides, etc.
3. GroupBy & Aggregation	Calculate mean trip distance or total fare per hour, per pickup zone, etc.
4. Datetime Handling	Convert pickup/dropoff columns to datetime objects; extract hours, weekdays, etc.
5. Pivot Tables/Crosstabs	Create tables showing trip count per day per hour, or fare amounts per vendor.
6. Text Data Processing	If you use the taxi zone lookup, you can explore zone names and clean text data.
7. Using NumPy	Perform vectorized calculations on fare amounts, trip durations, etc.
8. Data Read/Write	Read from CSVs (simulate F1/Spanner by writing to/from SQL or GSheets via API).
9. Pattern Recognition	Identify average fares by time of day, detect duplicate trips, analyze ride frequency.
10. Visualization	Plot histograms of fare amounts, heatmaps of pickup zones, correlations between features.
11. Optimization	Use chunking, Dask, or pyarrow to handle large files efficiently.
12. Gemini/GenAI (Optional)	Summarize trends, generate charts based on prompts (if you're using Gemini with Sheets/Colab).

---

### Data Extraction - Mounting drive with Colab. Uploading csv files to colab and importing using pandas
"""

from google.colab import drive
drive.mount('/content/drive')

"""## This file contains the Yellow Taxis Data for January 2015.
## Large Dataset Optimisation Techniques
* There are 4 csv datasets. We'll use one by one to
* Understand the structure
* Clean and analyze without heavy RAM usage
* Avoid early confusion from merging

Note: Once you finalize your workflow, apply it across the others and then combine them.

"""

# Imorting the dataset
import pandas as pd
df_1=pd.read_csv(r"/content/drive/MyDrive/NYC Taxi Large Dataset/yellow_tripdata_2015-01.csv")
df_1

df_1.shape

""":Details about Columns
* Vendor ID: code indicating the TPEP provider that provided the record.
* tpep_pickup_datetime:
The date and time when the meter was engaged.


* tpep_dropoff_datetime: The date and time when the meter was disengaged.


* passenger_count: The number of passengers in the vehicle. This is a driver-entered value.


* trip_distance: The elapsed trip distance in miles reported by the taximeter.


* pickup_longitude: Longitude where the meter was engaged.


* pickup_latitude: Latitude where the meter was engaged.


* RateCodeID: The final rate code in effect at the end of the trip.


* store_and_fwd_flag: This flag indicates whether the trip record was held in vehicle memory before sending to the vendor, aka “store and forward,” because the vehicle did not have a connection to the server.


* dropoff_longitude: Longitude where the meter was disengaged.

### Data Manipulation - Cleaning, sorting, filtering, joining, aggregating - "Making data ready for analysis"

Basic Info and Structure - "Peek at the Data"

Function	Description
1. df.head(n)	Shows the first n rows (default is 5)
2. df.tail(n)	Shows the last n rows
3. df.shape	Returns a tuple with (rows, columns)
4. df.info()	Displays index, column names, data types, and non-null counts
5. df.columns	Lists all column names
6. df.index	Shows the index (row labels)
7. df.dtypes	Displays data types of each column
8. df.memory_usage()	Shows memory usage of each column
"""

df_1.columns

df_1.describe()

df_1.info()

df_1.index

df_1.dtypes

df_1.memory_usage()

"""### Summary Statistics

Function	Description
1. df.describe()	Summary stats for numeric columns (count, mean, std, min, max, quartiles)
2. df.describe(include='all')	Summary stats designed to summarize all columns, not just numeric ones — including categorical (object/string), boolean, and datetime types.
Note : Metrics for String/Object columns are -
* count - all non-null values in the column
* unique - number of unique values in the column
* top - (mode) -most frequent value
* freq - count of most frequent value
3. df.mean(), df.median(), df.std()	Basic statistical measures
4. df.nunique()	Count of unique values in each column
5. df.value_counts()	Frequency count of unique values in a Series (use with df['column'])
6. df.mode()	Most frequent value(s) in each column
"""

df_1.describe() #summary stats only for numeric columns

df_1.describe(include='all')
# summary stats designed to summarize all columns- categorical (object/string), boolean, and datetime types.
# Metrics for string/object based columns - unique, top(mode), frequency

# find mean for only numeric columns (skipping string/object columns)

## Filtering

df_1_numeric=df_1.select_dtypes(include='number') # filters out only numeric columns

# Similarly, we can find median and std (Standard deviation) for numeric columns

print('mean of numeric columns',df_1_numeric.mean()) # finds mean of numeric columns
print('median of numeric columns',df_1_numeric.median()) # finds median of numeric columns
print('standard deviation of numeric columns',df_1_numeric.std()) # finds std of numeric columns

# Unique, Nunique and Value_counts
# Unique - Shows unique values - output (numpy array) - what are the categories - mainly for one column, doesn't go with entire dataset
# nunique - counts unique values - output (integer) - how many categories - counts unique values of all columns in a dataset
# value_counts - count how often each value occurs - full frequency distribution
# mode - most frequent values (if there's a tie, it returns all modes)

print('UNIQUE- unique values in the column are',df_1['VendorID'].unique())
print('NUNIQUE- number of unique values are',df_1['VendorID'].nunique())
print('VALUE_COUNTS - number of times each unique values occurs are',df_1['VendorID'].value_counts())
print('MODE - most frequent value in the column is',df_1['VendorID'].mode())
print('-----------------------------------------------------------')
# print('unique_values in the column - VendorID are',df_1['VendorID'].unique(), 'and count of unique values are', df_1['VendorID'].nunique())

(df_1['VendorID'].value_counts())

df_1['VendorID'].mode()

"""### Data Quality and Nulls
Function	Description
1. df.isnull().sum()	Count of missing values in each column
2. df.notnull().sum()	Count of non-null values
3. df.duplicated().sum()	Number of duplicated rows
4. df.duplicated().value_counts()	How many rows are duplicates vs unique
5. df.isna().any()	Check if any NA values exist per column

* Note: isnull() and isna() gives the same output
"""

df_1.isnull().sum()

df_1.notnull().sum()

df_1.duplicated().sum()

"""###### No duplicate values"""

df_1.duplicated()

df_1.duplicated().value_counts()

# df_1.duplicated() - returns a boolean Series (True if a row is a duplicate of a previous one, False if it's the first occurrence).
# True if duplicate and False if it's unique
# for example
import pandas as pd

df = pd.DataFrame({
    'name': ['Alice', 'Bob', 'Alice', 'Charlie', 'Bob'],
    'age': [25, 30, 25, 35, 30]
})

df

df.duplicated()

"""* 0    False  # First 'Alice',25 — unique
* 1    False  # First 'Bob',30 — unique
* 2     True  # Duplicate of index 0
* 3    False  # Unique row
* 4     True  # Duplicate of index 1


dtype: bool

"""

df.duplicated().value_counts()

"""* False    3   # Unique rows
* True     2   # Duplicated rows
* True means duplicate

#### Data Types and Conversion
* df.select_dtypes(include='number')	Select numeric columns
* df.select_dtypes(include='object')	Select object (string) columns
* df.astype('type')	                  Convert a column to a specific data type
"""

df_1.dtypes

df_numeric=df_1.select_dtypes(include='number').head()
df_numeric

df_object=df_1.select_dtypes(include='object').head()
df_object

# astype converts datatype of a column
# Here, vendorID is numeric datatype column, we changed it to string/object datatype
df_1['VendorID'].astype('object')

"""#### Data Sampling and Exploration
* df.sample(n)	Random sample of n rows
* df.corr()	Correlation between numeric features
* df.cov()	Covariance matrix
* df.unique()	Unique values in a Series (df['column'].unique())
* df['column'].value_counts(normalize=True)	Proportions of unique values
"""

df_1.sample(5) # random sample of 5 rows

# df.corr() - Correlation Matrix - Pairwise correlation between numeric columns in a dataframe
# Correlation coefficinet ranges from -1 to 1
# 1: Perfect correlation
# 0: No correlation
# -1: Perfect negative correlation
# --------------------------------------
# NOTE: correlation coefficient close to 1 or -1 = strong relationship
# --------------------------------------
# Methods Available:
# 'pearson' (default): Measures linear correlation.
# 'kendall': Measures ordinal association.
# 'spearman': Measures rank correlation.​

df_numeric=df_1.select_dtypes(include='number').head()  ### only the numeric columns
df_numeric.corr() # correlation between numeric features

# df.cov() — Covariance Matrix - Calculates the pairwise covariance between numeric columns in a DataFrame
# Covariance: Indicates the direction of the linear relationship between variables
# Positive covariance: Variables tend to increase together
# Negative covariance: One variable tends to increase when the other decrease

df_cov=df_numeric.cov() # covariance matrix
df_cov

df_1['VendorID'].value_counts(normalize=True)

# ​df['column'].value_counts(normalize=True)
# pandas function calculates the proportions (relative frequencies) of unique values in a Series
# understanding the distribution of categorical data
# it outputs - index and values

df = pd.DataFrame({
    'Fruit': ['Apple', 'Banana', 'Apple', 'Orange', 'Banana', 'Apple']
})

# Applying value_counts(normalize=True) to the 'Fruit' column:

df['Fruit'].value_counts(normalize=True)

"""This indicates that:​

* 'Apple' constitutes 50% of the entries.

* 'Banana' constitutes approximately 33.33%.

* 'Orange' constitutes approximately 16.67%.
"""

df['Fruit'].value_counts(normalize=True)*100 #displaying it in percentages

"""Use Cases
* Data Exploration: Understand the distribution of categorical variables.

* Data Cleaning: Identify and handle imbalanced classes.

* Feature Engineering: Create features based on the frequency of categories.

## df.apply(): Applying Functions to Rows or Columns
"""

# syntax : df.apply(func, axis=0, raw=False, result_type=None, args=(), **kwds)
# func : function to apply to each column or row
# 0 apply function to each column
# 1 apply function to each row

import pandas as pd
import numpy as np

df_app = pd.DataFrame({'A': [1, 2], 'B': [10, 20]})
df_app

# Square each element in the DataFrame

df_squared=df_app.apply(np.square)
df_squared

df_app

# Sum of elements in each row
# numpy (np) - numerical calculation using python
# function applies "row" vise - horizontally -|-|-|-|-> (like this)
# Row 0: 1+10=11 | Row 1: 2+20=22
row_sums = df_app.apply(np.sum, axis=1)
row_sums

df_app

# function applies "column" vise
# ----
# |
# ----
# |
# (like this) - vertically

col_sums = df_app.apply(np.sum, axis=0)
col_sums

# row 0: 1+2 = 3
# row 1: 10+20 = 30

df_app

# Applying a Custom Function with lambda

# Add 5 to each element
df_plus_five = df_app.apply(lambda x: x + 5)
df_plus_five

"""### df.groupby().agg(): Grouped Analysis"""

# The groupby() function is used to split the data into groups based on some criteria
# agg() is used to apply aggregation functions to these groups.
# df.groupby('column_name').agg({'col1': 'func1', 'col2': 'func2', ...})
# 'column_name': The column to group by.
# 'col1', 'col2', ...: Columns to aggregate.
# 'func1', 'func2', ...: Aggregation functions like 'sum', 'mean', 'count', etc

# Sample DataFrame
df_grp = pd.DataFrame({
    'Department': ['Sales', 'Sales', 'HR', 'HR'],
    'Salary': [50000, 60000, 45000, 52000],
    'Bonus': [5000, 6000, 4000, 4200]
})

df_grp

# Group by Department and calculate total Salary and total Bonus
# 'SUM' Function
df_grp.groupby('Department').agg({'Salary': 'sum', 'Bonus': 'sum'})

# Group by Department and calculate mean Salary and total Bonus
# 'MEAN' or 'AVERAGE' Function
df_grp.groupby('Department').agg({'Salary': 'mean', 'Bonus': 'sum'}) # Average Salary

df_grp

"""## pass a dictionary with multiple functions: More complex aggregation

"""

df_grp.groupby('Department').agg({'Salary': ['mean', 'max','min'], 'Bonus': ['sum', 'min','max']})

"""## **Clean The Data** - Data Wrangling, Handling Missing Values"""

df_1.head()

# check for missing values
df_1.isnull().sum()

# Displays only those columns that have at least one missing value
df_missing=df_1.isnull().sum()
print(df_missing[df_missing>0])

"""**Strategies for Handling Missing Data**

### Imputation Techniques - Replacing or Imputing missing values
"""

df_1.head()

# for numerical columns
# print(df_1['fare_amount'].isnull().sum())
# print(df_1['fare_amount'].dtype)
# df_1['fare_amount'].fillna(df_1['fare_amount'].median(), inplace=True)
df_1['fare_amount'] = df_1['fare_amount'].fillna(df_1['fare_amount'].median())
print(df_1['fare_amount'])

df_1['payment_type'].value_counts()

# for categorical columns
df_1['payment_type']=df_1['payment_type'].fillna(df_1['payment_type'].mode()[0]) # 0 is the first index number of most frequent value in the column
print(df_1['payment_type'])

"""* The mode() function in pandas returns the most frequently occurring value(s) in a Series.
* Unlike mean() or median(), which return a single value, mode() can return multiple values if there is a tie for the highest frequency.
* The result is a Series containing all modes in ascending order.
* Purpose of [0] - Using [0] selects the first mode from the Series returned by mode().

### Large Dataset + Small proportion of missing data = remove those enteries
* If greater proportion of missing values = this option might lose information, so use it, accordingly
"""

df_cleaned=df_1.dropna()
df_cleaned.isnull().sum()

"""## Date-Time Handling
* Handling datetime columns in a dataset is crucial for time-based analyses
* especially with datasets like the NYC Taxi dataset that include timestamps for pickups and drop-offs.
"""

df_1.head()

"""* pd.to_datetime() converts an argument—such as a string, integer, float, list, tuple, 1-D array, Series, or DataFrame/dict-like object—into a pandas datetime object.
* This conversion is essential for performing datetime operations like filtering by date ranges, extracting specific components (e.g., year, month), and resampling time series data.

"""

print(df_1['tpep_pickup_datetime'])
print(df_1['tpep_dropoff_datetime'])

# converts this to Pandas datetime datatype

df_1['tpep_pickup_datetime'] = pd.to_datetime(df_1['tpep_pickup_datetime'])
df_1['tpep_dropoff_datetime'] = pd.to_datetime(df_1['tpep_dropoff_datetime'])

df_1['tpep_pickup_datetime'].head()

"""* YYYY-MM-DD - Date
* HH:MM:SS - Time
"""

# After conversion, you can extract various components

# df_1['pickup_date'] = df_1['tpep_pickup_datetime'].dt.date
# df_1['pickup_date'].head(3)
# df_1['pickup_time'] = df_1['tpep_pickup_datetime'].dt.time
# df_1['pickup_time'].head(3)
# df_1['pickup_hour'] = df_1['tpep_pickup_datetime'].dt.hour
# df_1['pickup_hour'].head()
# df_1['pickup_day'] = df_1['tpep_pickup_datetime'].dt.day
# df_1['pickup_day'].head()
# df_1['pickup_weekday'] = df_1['tpep_pickup_datetime'].dt.day_name()
# df_1['pickup_weekday'].head(3)
# df_1['pickup_month'] = df_1['tpep_pickup_datetime'].dt.month
# df_1['pickup_month'].head(3)
# df_1['pickup_year'] = df_1['tpep_pickup_datetime'].dt.year
# df_1['pickup_year'].head(3)


## ---------------------------------------------------------------

print('DATE',df_1['tpep_pickup_datetime'].dt.date)
print('TIME',df_1['tpep_pickup_datetime'].dt.time)
print('HOUR',df_1['tpep_pickup_datetime'].dt.hour)
print('DAY',df_1['tpep_pickup_datetime'].dt.day)
print('YEAR',df_1['tpep_pickup_datetime'].dt.year)

## These components are useful for temporal analyses, such as identifying peak hours or seasonal trends.

"""* Trip Duration"""

df_1['trip_duration']=df_1['tpep_dropoff_datetime']-df_1['tpep_pickup_datetime']
df_1['trip_duration_minutes']=df_1['trip_duration'].dt.total_seconds()/60
df_1['trip_duration_minutes'].head()      # Trip duration in minutes

"""#### Filtering & Sorting

1. Filtering - Select rows with specific conditions
"""

# Filter by a single condition
filtered_df = df_1[df_1['trip_duration_minutes'] > 60]   # filter rows with trip_duration_time greater than 60 mins
filtered_df.head()

# rows/enteries with fare_amount>50
# syntax : new_variable=dataframe[dataframe['col] <> condition]

df_fare=df_1[df_1['fare_amount']>50]
df_fare.head()

# Filter by Multiple Conditions

filtered_df = df_1[(df_1['trip_duration_minutes'] > 60) & (df_1['fare_amount'] > 50) & (df_1['payment_type']==2)] # payment_type=2 (Cash)
filtered_df.head()

"""# # Filter Using .query() Method - A Great and readable way to filter data

"""

df_filtered = df_1.query("fare_amount > 50 and payment_type == 2 and tolls_amount > 3 and total_amount > 50 and trip_duration_minutes > 60")
df_filtered.head(6)

"""## Sorting - Sorting helps in organizing data based on column values."""

# Sort by a Single Column - To sort trips by fare amount in descending order - (Greater to Smaller)
# Syntax : df.sort_values(by = 'col_needs_to_be_sorted,ascending=True/False)

df_sorted = df_1.sort_values(by='fare_amount', ascending=False)
df_sorted.head(10)

sorted=df_1['fare_amount'].sort_values(ascending=False)
print(sorted)

# Sort by Multiple Columns

df_sorted_multiple= df_1.sort_values(by=['total_amount', 'fare_amount'], ascending=[True, False])
df_sorted_multiple

# ascending=[True, False] - This sorts the DataFrame by payment_type in ascending order and then by fare_amount in descending order.

"""### Combining Filtering and Sorting"""

top_cash_trips = df[df['payment_type'] == 'Cash'].sort_values(by='fare_amount', ascending=False).head(5)